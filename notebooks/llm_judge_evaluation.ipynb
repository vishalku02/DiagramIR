{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-Judge Evaluation\n",
    "\n",
    "This notebook runs the LLM-as-Judge pipeline end-to-end and inspects cached outputs under `results/llm_judge/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "repo_root = (notebook_dir / '..').resolve() if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "os.chdir(repo_root)\n",
    "print(f'Repo root: {repo_root}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = Path('data/geometric_shapes_test_set.csv')\n",
    "MODE = 'both'\n",
    "MODELS = 'gpt-4.1-mini'\n",
    "LIMIT = 10\n",
    "CONCURRENCY = 4\n",
    "RUN_PRECOMPUTE_PNGS = False\n",
    "\n",
    "assert DATASET.exists(), f'Missing dataset: {DATASET}'\n",
    "df = pd.read_csv(DATASET)\n",
    "required_cols = {'diagram_id', 'tikz'}\n",
    "missing_cols = sorted(required_cols - set(df.columns))\n",
    "if missing_cols:\n",
    "    raise ValueError(f'Missing required columns: {missing_cols}')\n",
    "print('Dataset:', DATASET)\n",
    "print('Rows:', len(df))\n",
    "print('Columns:', list(df.columns))\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PRECOMPUTE_PNGS:\n",
    "    cmd = [sys.executable, 'scripts/precompute_judge_pngs.py']\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "else:\n",
    "    print('Skipping PNG precompute (set RUN_PRECOMPUTE_PNGS=True to enable).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    sys.executable, 'llm_judge.py',\n",
    "    '--csv', str(DATASET),\n",
    "    '--mode', MODE,\n",
    "    '--models', MODELS,\n",
    "    '--limit', str(LIMIT),\n",
    "    '--concurrency', str(CONCURRENCY),\n",
    "]\n",
    "print('Running:', ' '.join(cmd))\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = Path('results/llm_judge') / MODE / MODELS.replace('/', '_')\n",
    "files = sorted(result_dir.glob('diagram_*.json'))\n",
    "print(f'Result directory: {result_dir}')\n",
    "print(f'Cached records: {len(files)}')\n",
    "if files:\n",
    "    sample = json.loads(files[0].read_text())\n",
    "    print('Sample keys:', sorted(sample.keys()))\n",
    "    print('Sample diagram_id:', sample.get('diagram_id'))\n",
    "    print('Sample mode/model:', sample.get('mode'), sample.get('model'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
