{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backtranslation Analysis\n",
        "\n",
        "Analysis Notebook for the results from backtranslation and LLM-as-a-judge experiments across different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import re\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from IR_model import TikzIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODELS = [\"gpt-5\", \"gpt-5-mini\", \"gpt-4.1\", \"gpt-4.1-mini\"] \n",
        "CACHE_DIR = Path(\"../results/backtranslation/cache\")\n",
        "\n",
        "EVALUATION_PATHS = {}\n",
        "\n",
        "for MODEL in MODELS:\n",
        "    EVALUATION_FILES = list(Path(\"../results/backtranslation/\").glob(f\"evaluation_results_{MODEL}_*.csv\"))\n",
        "    if EVALUATION_FILES:\n",
        "        EVALUATION_PATHS[MODEL] = EVALUATION_FILES[0]  \n",
        "        print(f\"Found evaluation file for {MODEL}: {EVALUATION_PATHS[MODEL]}\")\n",
        "    else:\n",
        "        print(f\"No evaluation file found for {MODEL}\")\n",
        "\n",
        "DATASET_PATH = Path(\"../data/geometric_shapes_test_set.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the original dataset to get prompts and metadata\n",
        "dataset = pd.read_csv(DATASET_PATH)\n",
        "print(f\"Dataset shape: {dataset.shape}\")\n",
        "print(f\"Dataset columns: {list(dataset.columns)}\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = {}\n",
        "\n",
        "for MODEL in MODELS:\n",
        "    if MODEL in EVALUATION_PATHS:\n",
        "        dfs[MODEL] = pd.read_csv(EVALUATION_PATHS[MODEL])\n",
        "        print(f\"{MODEL} strictly enforced IR count: {dfs[MODEL]['extraction_success'].sum()} / {len(dfs[MODEL])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"| Model | Correct IR Count |\")\n",
        "print(\"|-------|------------------|\")\n",
        "for MODEL in MODELS:\n",
        "    if MODEL in dfs:\n",
        "        success_count = dfs[MODEL]['extraction_success'].sum()\n",
        "        total_count = len(dfs[MODEL])\n",
        "        print(f\"| {MODEL} | {success_count} / {total_count} |\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for MODEL in MODELS:\n",
        "    if MODEL in dfs:\n",
        "        df = dfs[MODEL]\n",
        "        \n",
        "        if len(df) > 0:\n",
        "            # Overall metrics\n",
        "            avg_total_time_ms = (df['extraction_time_ms'] + df['evaluation_time_ms']).mean()\n",
        "            avg_total_time_s = avg_total_time_ms / 1000  # Convert to seconds\n",
        "            avg_cost = df['extraction_cost'].mean()\n",
        "            total_cost = df['extraction_cost'].sum()\n",
        "            \n",
        "            print(f\"{MODEL}:\")\n",
        "            print(f\"  Average time: {avg_total_time_s:.2f} s\")\n",
        "            # Break down by main category\n",
        "            for category in df['main_category'].unique():\n",
        "                category_df = df[df['main_category'] == category]\n",
        "                category_avg_time_ms = (category_df['extraction_time_ms'] + category_df['evaluation_time_ms']).mean()\n",
        "                category_avg_time_s = category_avg_time_ms / 1000\n",
        "                \n",
        "                print(f\"    {category}: {category_avg_time_s:.2f} s (n={len(category_df)})\")\n",
        "\n",
        "            print(f\"  Average cost: ${avg_cost:.4f}\")\n",
        "            print(f\"  Total cost: ${total_cost:.2f}\")\n",
        "            \n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Human Eval Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "irr_path = Path(\"../data/geometric_shapes_IRR_consensus.csv\")\n",
        "# calib_path = Path(\"../data/calib_set_consensus.csv\")\n",
        "vishal_set = Path(\"../data/vishal_geometric_shapes_test_set.csv\")\n",
        "rebecca_set = Path(\"../data/rebecca_geometric_shapes_test_set.csv\")\n",
        "shubhra_set = Path(\"../data/shubhra_geometric_shapes_test_set.csv\")\n",
        "\n",
        "\n",
        "irr = pd.read_csv(irr_path, keep_default_na=False, na_values=[''])\n",
        "# calib = pd.read_csv(calib_path, keep_default_na=False, na_values=['']) # used as our set for developing the checks and our calibration set for IRR\n",
        "vishal_df = pd.read_csv(vishal_set, keep_default_na=False, na_values=[''])\n",
        "rebecca_df = pd.read_csv(rebecca_set, keep_default_na=False, na_values=[''])\n",
        "shubhra_df = pd.read_csv(shubhra_set, keep_default_na=False, na_values=[''])\n",
        "\n",
        "human_eval_df = pd.concat([irr, vishal_df, rebecca_df, shubhra_df])\n",
        "\n",
        "\n",
        "print(f\"Human evaluation data shape: {human_eval_df.shape}\")\n",
        "print(\"Columns:\", human_eval_df.columns.tolist())\n",
        "\n",
        "print(\"\\nChecking applicable columns after fixed CSV reading:\")\n",
        "for col in human_eval_df.columns:\n",
        "    if col.startswith('Applicable'):\n",
        "        print(f\"\\n{col}:\")\n",
        "        unique_vals = human_eval_df[col].unique()\n",
        "        print(f\"  Unique values: {unique_vals}\")\n",
        "        value_counts = human_eval_df[col].value_counts()\n",
        "        for val, count in value_counts.items():\n",
        "            print(f\"    '{val}': {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify rating columns and separate those with/without 'Applicable' pairs\n",
        "metadata_cols = ['assigned_to', 'prompt', 'subcategory', 'image', 'tikz', 'Mathematical', 'Spatial']\n",
        "\n",
        "base_rating_cols = [col for col in human_eval_df.columns \n",
        "                   if col not in metadata_cols and not col.startswith('Applicable')]\n",
        "\n",
        "# Identify which columns have corresponding 'Applicable' columns\n",
        "cols_with_applicable = []\n",
        "standalone_cols = []\n",
        "\n",
        "for col in base_rating_cols:\n",
        "    applicable_col = f\"Applicable - {col}\"\n",
        "    if applicable_col in human_eval_df.columns:\n",
        "        cols_with_applicable.append(col)\n",
        "    else:\n",
        "        standalone_cols.append(col)\n",
        "\n",
        "print(\"Columns with 'Applicable' option:\")\n",
        "for col in cols_with_applicable:\n",
        "    print(f\"  - {col}\")\n",
        "    \n",
        "print(f\"\\nStandalone columns:\")\n",
        "for col in standalone_cols:\n",
        "    print(f\"  - {col}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to collapse a main column with its applicable column\n",
        "def collapse_applicable_column(df, main_col, applicable_col):\n",
        "    \"\"\"\n",
        "    Collapse main column and applicable column into a single column with Yes/No/N/A values\n",
        "    If applicable column is 'N/A', the result is N/A\n",
        "    Otherwise, use the main column value\n",
        "    \"\"\"\n",
        "    result = df[main_col].copy()\n",
        "    \n",
        "    # Where applicable column indicates N/A, set result to N/A\n",
        "    if applicable_col in df.columns:\n",
        "        applicable_mask = df[applicable_col] == 'N/A'\n",
        "        result.loc[applicable_mask] = 'N/A'\n",
        "    \n",
        "    return result\n",
        "\n",
        "collapsed_human_data = {}\n",
        "\n",
        "for col in standalone_cols:\n",
        "    collapsed_human_data[col] = human_eval_df[col]\n",
        "\n",
        "for col in cols_with_applicable:\n",
        "    applicable_col = f\"Applicable - {col}\"\n",
        "    collapsed_human_data[col] = collapse_applicable_column(human_eval_df, col, applicable_col)\n",
        "\n",
        "tikz_series = human_eval_df['tikz'].astype(str).str.strip() if 'tikz' in human_eval_df.columns else pd.Series([''] * len(human_eval_df))\n",
        "human_eval_df = pd.DataFrame({\n",
        "    'prompt': human_eval_df['prompt'].astype(str).str.strip(),\n",
        "    'subcategory': human_eval_df['subcategory'],\n",
        "    'tikz': tikz_series,\n",
        "    **collapsed_human_data\n",
        "})\n",
        "\n",
        "print(f\"Processed human evaluation data shape: {human_eval_df.shape}\")\n",
        "print(\"Available rating columns:\", list(collapsed_human_data.keys()))\n",
        "\n",
        "print(\"\\nUnique values after collapse:\")\n",
        "for col in ['Labeled angles (if any) match drawn angle', 'Labeled lengths and areas (if any) match visual proportions', 'Labels (if any) are associated with correct elements']:\n",
        "    if col in human_eval_df.columns:\n",
        "        print(f\"\\n{col}:\")\n",
        "        value_counts = human_eval_df[col].value_counts()\n",
        "        for val, count in value_counts.items():\n",
        "            print(f\"  {val}: {count}\")\n",
        "\n",
        "# Drop duplicate diagram_id rows \n",
        "if 'human_eval_with_ids' in globals():\n",
        "    human_eval_with_ids = human_eval_with_ids.drop_duplicates('diagram_id', keep='first')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the original dataset to match prompts and TikZ code\n",
        "DATASET_PATH = Path(\"../data/geometric_shapes_test_set.csv\")\n",
        "original_dataset = pd.read_csv(DATASET_PATH, keep_default_na=False, na_values=[''])\n",
        "original_dataset['prompt'] = original_dataset['prompt'].astype(str).str.strip()\n",
        "if 'tikz' in original_dataset.columns:\n",
        "    original_dataset['tikz'] = original_dataset['tikz'].astype(str).str.strip()\n",
        "else:\n",
        "    original_dataset['tikz'] = ''\n",
        "human_eval_with_ids = human_eval_df.merge(\n",
        "    original_dataset[['diagram_id', 'prompt', 'tikz']],\n",
        "    on=['prompt', 'tikz'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# due to heavy class imbalance, omitted shape_outlines_are_closed_passed and core_mathematical_properties_of_shapes_correct_passed\n",
        "human_eval_with_ids = human_eval_with_ids.drop_duplicates('diagram_id', keep='first')\n",
        "column_mapping = {\n",
        "    # \"Shape(s) is closed\": \"shape_outlines_are_closed_passed\",\n",
        "    # \"Core math properties of the shape(s) are correct\": \"core_mathematical_properties_of_shapes_correct_passed\",\n",
        "    \"Diagram is fully in frame\": \"diagram_fully_in_canvas_passed\",\n",
        "    \"Diagram elements are scaled to be readable\": \"diagram_elements_are_readable_size_passed\",\n",
        "    \"Diagram elements don't problematically overlap\": \"diagram_elements_dont_problematically_overlap_passed\",\n",
        "    \"Labeled angles (if any) match drawn angle\": \"angle_labels_matches_arcs_passed\",\n",
        "    \"Labeled lengths and areas (if any) match visual proportions\": \"labeled_lengths_areas_match_proportions_passed\",\n",
        "    \"Labels (if any) are associated with correct elements\": \"labels_associated_with_elements_passed\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "def calculate_cohen_kappa_agreement(human_eval_with_ids, model_df, column_mapping):\n",
        "    \n",
        "    # Merge human eval with model results on diagram_id\n",
        "    merged = human_eval_with_ids.merge(\n",
        "        model_df, \n",
        "        on='diagram_id', \n",
        "        how='inner'\n",
        "    )\n",
        "    \n",
        "    print(f\"Merged {len(merged)} examples for comparison\")\n",
        "    \n",
        "    if len(merged) < 2:\n",
        "        print(\"Too few examples for meaningful comparison\")\n",
        "        return {}\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for human_col, auto_col in column_mapping.items():\n",
        "        human_vals = merged[human_col].tolist()\n",
        "        auto_vals = merged[auto_col].tolist()\n",
        "        \n",
        "        # Standardize auto values (True/False -> Yes/No, keep N/A as is)\n",
        "        auto_standardized = []\n",
        "        for val in auto_vals:\n",
        "            if pd.isna(val):\n",
        "                auto_standardized.append('N/A')\n",
        "            elif isinstance(val, str):\n",
        "                auto_standardized.append(val)\n",
        "            elif val is True:\n",
        "                auto_standardized.append('Yes')\n",
        "            elif val is False:\n",
        "                auto_standardized.append('No')\n",
        "            else:\n",
        "                auto_standardized.append(str(val))\n",
        "        \n",
        "        try:\n",
        "            # Calculate Cohen's Kappa with explicit labels\n",
        "            labels = ['Yes', 'No', 'N/A']\n",
        "            kappa = cohen_kappa_score(human_vals, auto_standardized, labels=labels)\n",
        "            \n",
        "            # Get distributions for context\n",
        "            human_dist = pd.Series(human_vals).value_counts().to_dict()\n",
        "            auto_dist = pd.Series(auto_standardized).value_counts().to_dict()\n",
        "            \n",
        "            results[human_col] = {\n",
        "                'kappa': kappa,\n",
        "                'n_examples': len(human_vals),\n",
        "                'human_distribution': human_dist,\n",
        "                'auto_distribution': auto_dist\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            results[human_col] = {\n",
        "                'kappa': None,\n",
        "                'n_examples': len(human_vals),\n",
        "                'error': str(e)\n",
        "            }\n",
        "    \n",
        "    return results\n",
        "# Calculate Cohen's Kappa for each model\n",
        "print(\"COHEN'S KAPPA ANALYSIS:\")\n",
        "print(\"=\"*60)\n",
        "all_kappa_results = {}\n",
        "for MODEL in MODELS:\n",
        "    if MODEL not in dfs:\n",
        "        continue\n",
        "    \n",
        "    print(f\"\\n{MODEL}:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    kappa_results = calculate_cohen_kappa_agreement(\n",
        "        human_eval_with_ids, \n",
        "        dfs[MODEL], \n",
        "        column_mapping\n",
        "    )\n",
        "    \n",
        "    all_kappa_results[MODEL] = kappa_results\n",
        "    \n",
        "    # Print results for this model\n",
        "    for check, result in kappa_results.items():\n",
        "        print(f\"\\nCHECK: {check}\")\n",
        "        if result['kappa'] is not None:\n",
        "            print(f\"  Cohen's Kappa: {result['kappa']:.3f}\")\n",
        "            print(f\"  Examples: {result['n_examples']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ordered_rubrics = [\n",
        "    (\"Shape(s) is closed\", \"Shapes closed\"),\n",
        "    (\"Core math properties of the shape(s) are correct\", \"Core math correct\"),\n",
        "    (\"Diagram is fully in frame\", \"Diagram fully in canvas\"),\n",
        "    (\"Diagram elements are scaled to be readable\", \"Elements are readable size\"),\n",
        "    (\"Diagram elements don't problematically overlap\", \"Elements do not problematically overlap\"),\n",
        "    (\"Labeled angles (if any) match drawn angle\", \"Angle labels match arcs\"),\n",
        "    (\"Labeled lengths and areas (if any) match visual proportions\", \"Labeled lengths/areas match proportions\"),\n",
        "    (\"Labels (if any) are associated with correct elements\", \"Labels associated with elements\"),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for raw_name, pretty_name in ordered_rubrics:\n",
        "    series = human_eval_df[raw_name].astype(str).str.strip()\n",
        "    total = len(series)\n",
        "\n",
        "    yes = (series == \"Yes\").sum()\n",
        "    no = (series == \"No\").sum()\n",
        "    na = (series == \"N/A\").sum()\n",
        "\n",
        "    rows.append({\n",
        "        \"Rubric\": pretty_name,\n",
        "        \"Yes (#)\": yes,\n",
        "        \"Yes (%)\": yes / total,\n",
        "        \"No (#)\": no,\n",
        "        \"No (%)\": no / total,\n",
        "        \"N/A (#)\": na,\n",
        "        \"N/A (%)\": na / total,\n",
        "        \"Total\": total,\n",
        "    })\n",
        "\n",
        "human_error_stats = (\n",
        "    pd.DataFrame(rows)\n",
        "      .set_index(\"Rubric\")\n",
        "      .round({\"Yes (%)\": 3, \"No (%)\": 3, \"N/A (%)\": 3})\n",
        ")\n",
        "\n",
        "print(\"Human annotation distribution with counts and percentages:\")\n",
        "display(human_error_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLM as a judge "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper functions for LLM Judge analysis\n",
        "import json\n",
        "from typing import Dict, List\n",
        "\n",
        "MODEL_COSTS = {\n",
        "    'gpt-5':        (1.25, 10.00),\n",
        "    'gpt-5-mini':   (0.25, 2.00),\n",
        "    'gpt-4.1':      (2.00, 8.00),\n",
        "    'gpt-4.1-mini': (0.40, 1.60),\n",
        "}\n",
        "CACHE_DISCOUNT = 0.25  # cached prompt tokens are billed at 25% of the prompt rate\n",
        "\n",
        "def calculate_judge_cost(tokens: Dict[str, int], model: str, *, include_cache_discount: bool = False) -> float:\n",
        "    \"\"\"Estimate API cost from token usage. Optionally ignore cache discount.\"\"\"\n",
        "    if model not in MODEL_COSTS:\n",
        "        raise ValueError(f'Unknown model: {model}')\n",
        "    prompt_cost_per_million, completion_cost_per_million = MODEL_COSTS[model]\n",
        "    prompt_rate = prompt_cost_per_million / 1_000_000\n",
        "    cached_rate = (prompt_rate * CACHE_DISCOUNT) if include_cache_discount else prompt_rate\n",
        "    completion_rate = completion_cost_per_million / 1_000_000\n",
        "\n",
        "    input_tokens = tokens.get('input_tokens') or 0\n",
        "    cached_tokens = tokens.get('cached_tokens') or 0\n",
        "    cached_tokens = min(cached_tokens, input_tokens)\n",
        "    fresh_tokens = input_tokens - cached_tokens\n",
        "    completion_tokens = tokens.get('output_tokens') or 0\n",
        "\n",
        "    return (fresh_tokens * prompt_rate) + (cached_tokens * cached_rate) + (completion_tokens * completion_rate)\n",
        "\n",
        "def calculate_judge_cost_no_cache(tokens: Dict[str, int], model: str) -> float:\n",
        "    \"\"\"Cost if cached tokens are billed at the full prompt rate.\"\"\"\n",
        "    return calculate_judge_cost(tokens, model, include_cache_discount=False)\n",
        "\n",
        "def load_judge_records(results_root: Path) -> pd.DataFrame:\n",
        "    records: List[Dict[str, object]] = []\n",
        "    for mode_dir in results_root.iterdir():\n",
        "        if not mode_dir.is_dir():\n",
        "            continue\n",
        "        mode = mode_dir.name\n",
        "        for model_dir in mode_dir.iterdir():\n",
        "            if not model_dir.is_dir():\n",
        "                continue\n",
        "            model = model_dir.name\n",
        "            for path in model_dir.glob('diagram_*.json'):\n",
        "                data = json.loads(path.read_text())\n",
        "                tokens = data.get('tokens', {})\n",
        "                record: Dict[str, object] = {\n",
        "                    'diagram_id': data.get('diagram_id'),\n",
        "                    'mode': data.get('mode', mode),\n",
        "                    'model': data.get('model', model),\n",
        "                    'temperature': data.get('temperature'),\n",
        "                    'reasoning_effort': data.get('reasoning_effort'),\n",
        "                    'elapsed_ms': data.get('elapsed_ms'),\n",
        "                    'cost': calculate_judge_cost(tokens, data.get('model', model)),\n",
        "                    'cost_no_cache': calculate_judge_cost_no_cache(tokens, data.get('model', model)),\n",
        "                    'input_tokens': tokens.get('input_tokens'),\n",
        "                    'cached_tokens': tokens.get('cached_tokens'),\n",
        "                    'output_tokens': tokens.get('output_tokens'),\n",
        "                    'total_tokens': tokens.get('total_tokens'),\n",
        "                }\n",
        "                rubric = data.get('rubric', {}) or {}\n",
        "                for rubric_key, rubric_payload in rubric.items():\n",
        "                    value = None\n",
        "                    if isinstance(rubric_payload, dict):\n",
        "                        value = rubric_payload.get('value')\n",
        "                    elif isinstance(rubric_payload, str):\n",
        "                        value = rubric_payload\n",
        "                    record[f'rubric::{rubric_key}'] = value\n",
        "                records.append(record)\n",
        "    if not records:\n",
        "        raise RuntimeError(f'No judge records found under {results_root}')\n",
        "    df = pd.DataFrame(records)\n",
        "    df.sort_values(['mode', 'model', 'diagram_id'], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "print(\"Helper functions for LLM Judge analysis loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Judge-specific agreement helpers\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, f1_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_judge_cohen_kappa_agreement(human_eval_with_ids, judge_df, column_mapping):\n",
        "    judge_df_copy = judge_df.copy()\n",
        "    judge_df_copy['diagram_id'] = judge_df_copy['diagram_id'].astype(str)\n",
        "    judge_df_copy = judge_df_copy.drop_duplicates('diagram_id', keep='first')\n",
        "\n",
        "    human_eval_copy = human_eval_with_ids.copy()\n",
        "    human_eval_copy['diagram_id'] = human_eval_copy['diagram_id'].astype(str)\n",
        "    human_eval_copy = human_eval_copy.drop_duplicates('diagram_id', keep='first')\n",
        "\n",
        "    merged = human_eval_copy.merge(judge_df_copy, on='diagram_id', how='inner')\n",
        "    merged = merged.drop_duplicates('diagram_id', keep='first')\n",
        "\n",
        "    print(f\"Merged {len(merged)} examples for comparison\")\n",
        "    if len(merged) < 2:\n",
        "        print(\"Too few examples for meaningful comparison\")\n",
        "        return {}\n",
        "\n",
        "    results = {}\n",
        "    for human_col, judge_col in column_mapping.items():\n",
        "        human_vals = merged[human_col].tolist()\n",
        "        judge_vals = merged[judge_col].tolist()\n",
        "\n",
        "        # Normalize auto labels\n",
        "        auto_standardized = []\n",
        "        for val in judge_vals:\n",
        "            if pd.isna(val):\n",
        "                auto_standardized.append('N/A')\n",
        "            elif isinstance(val, str):\n",
        "                auto_standardized.append(val)\n",
        "            elif val is True:\n",
        "                auto_standardized.append('Yes')\n",
        "            elif val is False:\n",
        "                auto_standardized.append('No')\n",
        "            else:\n",
        "                auto_standardized.append(str(val))\n",
        "\n",
        "        labels = ['Yes', 'No', 'N/A']\n",
        "        kappa = cohen_kappa_score(human_vals, auto_standardized, labels=labels)\n",
        "        human_dist = pd.Series(human_vals).value_counts().to_dict()\n",
        "        judge_dist = pd.Series(auto_standardized).value_counts().to_dict()\n",
        "\n",
        "        results[human_col] = {\n",
        "            'kappa': kappa,\n",
        "            'n_examples': len(human_vals),\n",
        "            'human_distribution': human_dist,\n",
        "            'judge_distribution': judge_dist,\n",
        "        }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "judge_results_root = Path(\"../results/llm_judge\")\n",
        "judge_df = load_judge_records(judge_results_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "judge_column_mapping = {\n",
        "    \"Diagram is fully in frame\": \"rubric::diagram_fully_in_canvas\",\n",
        "    \"Diagram elements are scaled to be readable\": \"rubric::diagram_elements_are_readable_size\",\n",
        "    \"Diagram elements don't problematically overlap\": \"rubric::diagram_elements_dont_problematically_overlap\",\n",
        "    \"Labeled angles (if any) match drawn angle\": \"rubric::angle_labels_matches_arcs\",\n",
        "    \"Labeled lengths and areas (if any) match visual proportions\": \"rubric::labeled_lengths_areas_match_proportions\",\n",
        "    \"Labels (if any) are associated with correct elements\": \"rubric::labels_associated_with_elements\",\n",
        "}\n",
        "\n",
        "judge_modes = sorted(judge_df[\"mode\"].unique())\n",
        "all_judge_kappa_results = {mode: {} for mode in judge_modes}\n",
        "all_judge_pr_results = {mode: {} for mode in judge_modes}\n",
        "\n",
        "for mode in judge_modes:\n",
        "    mode_df = judge_df[judge_df[\"mode\"] == mode]\n",
        "    for MODEL in [\"gpt-4.1\", \"gpt-5\", \"gpt-4.1-mini\", \"gpt-5-mini\"]:\n",
        "        model_df = mode_df[mode_df[\"model\"] == MODEL].copy()\n",
        "        if model_df.empty:\n",
        "            continue\n",
        "\n",
        "        kappa_results = calculate_judge_cohen_kappa_agreement(\n",
        "            human_eval_with_ids, model_df, judge_column_mapping\n",
        "        )\n",
        "        all_judge_kappa_results[mode][MODEL] = kappa_results\n",
        "\n",
        "        pr_results = calculate_judge_precision_recall_metrics(\n",
        "            human_eval_with_ids, model_df, judge_column_mapping\n",
        "        )\n",
        "        all_judge_pr_results[mode][MODEL] = pr_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rubrics = [\n",
        "    \"Labeled angles (if any) match drawn angle\",\n",
        "    \"Labeled lengths and areas (if any) match visual proportions\",\n",
        "    \"Diagram is fully in frame\",\n",
        "    \"Diagram elements are scaled to be readable\",\n",
        "    \"Labels (if any) are associated with correct elements\",\n",
        "    \"Diagram elements don't problematically overlap\",\n",
        "]\n",
        "\n",
        "friendly = {\n",
        "    \"gpt-4.1\": \"gpt 4.1\",\n",
        "    \"gpt-4.1-mini\": \"gpt 4.1 mini\",\n",
        "    \"gpt-5\": \"gpt 5\",\n",
        "    \"gpt-5-mini\": \"gpt 5 mini\",\n",
        "}\n",
        "\n",
        "summary_rows = []\n",
        "for rubric in rubrics:\n",
        "    row = {\"rubric\": rubric}\n",
        "\n",
        "    # Backtranslation K\n",
        "    for model in [\"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-5\", \"gpt-5-mini\"]:\n",
        "        kappa_val = all_kappa_results.get(model, {}).get(rubric, {}).get(\"kappa\")\n",
        "        row[(\"Backtranslation\", friendly[model])] = kappa_val\n",
        "\n",
        "    # Judge K per mode\n",
        "    for mode in sorted(judge_df[\"mode\"].unique()):\n",
        "        mode_results = all_judge_kappa_results.get(mode, {})\n",
        "        for model in [\"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-5\", \"gpt-5-mini\"]:\n",
        "            kappa_val = mode_results.get(model, {}).get(rubric, {}).get(\"kappa\")\n",
        "            row[(f\"Judge – {mode}\", friendly[model])] = kappa_val\n",
        "\n",
        "    summary_rows.append(row)\n",
        "\n",
        "kappa_table_only = pd.DataFrame(summary_rows).set_index(\"rubric\")\n",
        "kappa_table_only.columns = pd.MultiIndex.from_tuples(kappa_table_only.columns)\n",
        "print(\"\\nκ comparisons (Backtranslation vs LLM-as-a-Judge modes):\")\n",
        "display(kappa_table_only.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_back = [(\"Backtranslation\", name) for name in friendly.values()]\n",
        "\n",
        "# Backtranslation vs Judge (both mode)\n",
        "back_vs_both = kappa_table_only.loc[\n",
        "    :, cols_back + [(\"Judge – both\", name) for name in friendly.values()]\n",
        "]\n",
        "print(\"\\nκ comparison: Backtranslation vs LLM-as-a-Judge (both mode)\")\n",
        "back_vs_both_styled = back_vs_both.style.highlight_max(axis=1, props=\"font-weight: bold;\")\n",
        "display(back_vs_both_styled)\n",
        "\n",
        "\n",
        "# Backtranslation vs Judge (code mode)\n",
        "back_vs_code = kappa_table_only.loc[\n",
        "    :, cols_back + [(\"Judge – code\", name) for name in friendly.values()]\n",
        "]\n",
        "print(\"\\nκ comparison: Backtranslation vs LLM-as-a-Judge (code mode)\")\n",
        "back_vs_code_styled = back_vs_code.style.highlight_max(axis=1, props=\"font-weight: bold;\")\n",
        "display(back_vs_code_styled)\n",
        "\n",
        "# Backtranslation vs Judge (image mode)\n",
        "back_vs_image = kappa_table_only.loc[\n",
        "    :, cols_back + [(\"Judge – image\", name) for name in friendly.values()]\n",
        "]\n",
        "print(\"\\nκ comparison: Backtranslation vs LLM-as-a-Judge (image mode)\")\n",
        "back_vs_image_styled = back_vs_image.style.highlight_max(axis=1, props=\"font-weight: bold;\")\n",
        "display(back_vs_image_styled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols_back = [(\"Backtranslation\", name) for name in friendly.values()]\n",
        "cols_both = [(\"Judge – both\", name) for name in friendly.values()]\n",
        "cols_code = [(\"Judge – code\", name) for name in friendly.values()]\n",
        "cols_image = [(\"Judge – image\", name) for name in friendly.values()]\n",
        "\n",
        "single_row = pd.DataFrame({\n",
        "    \"Backtranslation – κ\": back_vs_both[cols_back].mean(skipna=True).values,\n",
        "    \"Judge – both – κ\": back_vs_both[cols_both].mean(skipna=True).values,\n",
        "    \"Judge – code – κ\": back_vs_code[cols_code].mean(skipna=True).values,\n",
        "    \"Judge – image – κ\": back_vs_image[cols_image].mean(skipna=True).values,\n",
        "}, index=friendly.values())\n",
        "\n",
        "print(\"Average κ across all rubric checks (columns = baseline vs each judge mode)\")\n",
        "display(single_row.round(3).style.highlight_max(axis=1, props=\"font-weight: bold;\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Backtranslation summary\n",
        "bt_summary_rows = []\n",
        "for MODEL in MODELS:\n",
        "    if MODEL not in all_kappa_results:\n",
        "        continue\n",
        "    kappa_results = all_kappa_results[MODEL]\n",
        "    kappas = [res['kappa'] for res in kappa_results.values() if res.get('kappa') is not None]\n",
        "    avg_kappa = sum(kappas) / len(kappas) if kappas else float('nan')\n",
        "    bt_summary_rows.append({\n",
        "        'model': friendly[MODEL], \n",
        "        'cohen_k': avg_kappa,\n",
        "    })\n",
        "bt_summary_df = pd.DataFrame(bt_summary_rows)\n",
        "\n",
        "# Judge summary (all modes)\n",
        "judge_summary_rows = []\n",
        "for mode in judge_modes:\n",
        "    for MODEL in MODELS:\n",
        "        kappa_results = all_judge_kappa_results.get(mode, {}).get(MODEL)\n",
        "        if not kappa_results:\n",
        "            continue\n",
        "        kappas = [res[\"kappa\"] for res in kappa_results.values() if res.get(\"kappa\") is not None]\n",
        "        avg_kappa = sum(kappas) / len(kappas) if kappas else float(\"nan\")\n",
        "\n",
        "        judge_summary_rows.append({\n",
        "            \"mode\": mode,\n",
        "            \"model\": friendly[MODEL], \n",
        "            \"cohen_k\": avg_kappa,\n",
        "        })\n",
        "judge_summary_df = pd.DataFrame(judge_summary_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_backtranslation_stats(model_id):\n",
        "    df = dfs[model_id]\n",
        "    avg_time_s = ((df[\"extraction_time_ms\"] + df[\"evaluation_time_ms\"]).mean()) / 1000\n",
        "    total_cost = df[\"extraction_cost\"].sum()\n",
        "    avg_kappa = bt_summary_df.set_index(\"model\").loc[friendly[model_id], \"cohen_k\"]\n",
        "    return avg_kappa, avg_time_s, total_cost\n",
        "\n",
        "def compute_judge_stats(mode, model_id):\n",
        "    df = judge_df[(judge_df[\"mode\"] == mode) & (judge_df[\"model\"] == model_id)]\n",
        "    avg_time_s = df[\"elapsed_ms\"].mean() / 1000\n",
        "    total_cost = df[\"cost\"].sum()\n",
        "    avg_kappa = judge_summary_df.set_index([\"mode\", \"model\"]).loc[(mode, friendly[model_id]), \"cohen_k\"]\n",
        "    return avg_kappa, avg_time_s, total_cost\n",
        "\n",
        "comparison_tables = {}\n",
        "\n",
        "for mode in sorted(judge_df[\"mode\"].unique()):\n",
        "    rows = []\n",
        "    for model_id in [\"gpt-4.1\", \"gpt-5\", \"gpt-4.1-mini\", \"gpt-5-mini\"]:\n",
        "        bt_k, bt_time, bt_cost = compute_backtranslation_stats(model_id)\n",
        "        judge_k, judge_time, judge_cost = compute_judge_stats(mode, model_id)\n",
        "        rows.append({\n",
        "            \"Model\": friendly[model_id],\n",
        "            \"BT κ\": bt_k,\n",
        "            \"BT time (s)\": bt_time,\n",
        "            \"BT cost ($)\": bt_cost,\n",
        "            f\"Judge ({mode}) κ\": judge_k,\n",
        "            f\"Judge ({mode}) time (s)\": judge_time,\n",
        "            f\"Judge ({mode}) cost ($)\": judge_cost,\n",
        "        })\n",
        "\n",
        "    comparison_tables[mode] = (\n",
        "        pd.DataFrame(rows)\n",
        "        .set_index(\"Model\")\n",
        "        .round({\"BT κ\": 3, \"BT time (s)\": 2, \"BT cost ($)\": 2,\n",
        "                f\"Judge ({mode}) κ\": 3, f\"Judge ({mode}) time (s)\": 2, f\"Judge ({mode}) cost ($)\": 2})\n",
        "    )\n",
        "\n",
        "for mode, table in comparison_tables.items():\n",
        "    print(f\"\\nBacktranslation vs LLM-as-a-Judge ({mode})\")\n",
        "    display(table)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
