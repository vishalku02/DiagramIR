{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Agreement Analysis: Backtranslation vs LLM-as-Judge\n",
    "\n",
    "This notebook compares results from both approaches across models: backtranslation and LLM-as-Judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "\n",
    "if not (REPO_ROOT / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "\n",
    "# for normalizing the labels across different datasets\n",
    "label_map = {\n",
    "    \"yes\": \"Yes\", \"y\": \"Yes\", \"true\": \"Yes\", \"1\": \"Yes\",\n",
    "    \"no\": \"No\", \"n\": \"No\", \"false\": \"No\", \"0\": \"No\",\n",
    "    \"n/a\": \"N/A\", \"na\": \"N/A\", \"not applicable\": \"N/A\", \"\": \"N/A\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found evaluation file for gpt-5: /Users/vishalkumar/Desktop/Research/code/DiagramIR/results/backtranslation/evaluation_results_gpt-5_20250924.csv\n",
      "Found evaluation file for gpt-5-mini: /Users/vishalkumar/Desktop/Research/code/DiagramIR/results/backtranslation/evaluation_results_gpt-5-mini_20250924.csv\n",
      "Found evaluation file for gpt-4.1: /Users/vishalkumar/Desktop/Research/code/DiagramIR/results/backtranslation/evaluation_results_gpt-4.1_20250924.csv\n",
      "Found evaluation file for gpt-4.1-mini: /Users/vishalkumar/Desktop/Research/code/DiagramIR/results/backtranslation/evaluation_results_gpt-4.1-mini_20260219.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODELS = [\"gpt-5\", \"gpt-5-mini\", \"gpt-4.1\", \"gpt-4.1-mini\"]\n",
    "BACKTRANSLATION_RESULTS_ROOT = REPO_ROOT / \"results\" / \"backtranslation\"\n",
    "\n",
    "EVALUATION_PATHS = {}\n",
    "\n",
    "for MODEL in MODELS:\n",
    "    EVALUATION_FILES = list(BACKTRANSLATION_RESULTS_ROOT.glob(f\"evaluation_results_{MODEL}_*.csv\"))\n",
    "    if EVALUATION_FILES:\n",
    "        EVALUATION_PATHS[MODEL] = EVALUATION_FILES[0]\n",
    "        print(f\"Found evaluation file for {MODEL}: {EVALUATION_PATHS[MODEL]}\")\n",
    "    else:\n",
    "        print(f\"No evaluation file found for {MODEL}\")\n",
    "\n",
    "DATASET_PATH = REPO_ROOT / \"data\" / \"geometric_shapes_test_set.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll load the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (398, 9)\n",
      "Dataset columns: ['prompt', 'tikz', 'image', 'main_category', 'subcategory', 'diagram_id', 'assignment_type', 'assigned_to', 'image_png_path']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>tikz</th>\n",
       "      <th>image</th>\n",
       "      <th>main_category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>diagram_id</th>\n",
       "      <th>assignment_type</th>\n",
       "      <th>assigned_to</th>\n",
       "      <th>image_png_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triangle with side length 8 horizontal at bott...</td>\n",
       "      <td>\\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...</td>\n",
       "      <td>https://2xavun1dsa0sayar.public.blob.vercel-st...</td>\n",
       "      <td>2d shapes</td>\n",
       "      <td>triangle</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "      <td>Shubhra</td>\n",
       "      <td>data/judge_pngs/diagram_1.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  triangle with side length 8 horizontal at bott...   \n",
       "\n",
       "                                                tikz  \\\n",
       "0  \\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...   \n",
       "\n",
       "                                               image main_category  \\\n",
       "0  https://2xavun1dsa0sayar.public.blob.vercel-st...     2d shapes   \n",
       "\n",
       "  subcategory  diagram_id assignment_type assigned_to  \\\n",
       "0    triangle           1      individual     Shubhra   \n",
       "\n",
       "                  image_png_path  \n",
       "0  data/judge_pngs/diagram_1.png  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "print(f\"Dataset columns: {list(dataset.columns)}\")\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load and shape the human annotations, necessary for calculating human agreement across both auto-eval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human eval rows (raw): 386\n",
      "Human eval rows matched to dataset: 386\n"
     ]
    }
   ],
   "source": [
    "human_eval_raw = pd.read_csv(REPO_ROOT / \"data\" / \"human_ratings.csv\", keep_default_na=False, na_values=[\"\"])\n",
    "original_dataset = pd.read_csv(DATASET_PATH, keep_default_na=False, na_values=[\"\"])\n",
    "\n",
    "# Normalize join keys\n",
    "for df in (human_eval_raw, original_dataset):\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str).str.strip()\n",
    "    df[\"tikz\"] = df.get(\"tikz\", \"\").astype(str).str.strip() if \"tikz\" in df.columns else \"\"\n",
    "\n",
    "# Collapse \"Applicable - X\": if not applicable, force X -> \"N/A\"\n",
    "metadata_cols = {\"assigned_to\", \"prompt\", \"subcategory\", \"image\", \"tikz\", \"Mathematical\", \"Spatial\", \"diagram_id\", \"rating_source\"}\n",
    "rating_cols = [c for c in human_eval_raw.columns if c not in metadata_cols and not c.startswith(\"Applicable - \")]\n",
    "\n",
    "human_eval_df = human_eval_raw[[\"prompt\", \"subcategory\", \"tikz\"] + rating_cols].copy()\n",
    "for col in rating_cols:\n",
    "    app_col = f\"Applicable - {col}\"\n",
    "    if app_col in human_eval_raw.columns:\n",
    "        human_eval_df.loc[human_eval_raw[app_col].eq(\"N/A\"), col] = \"N/A\"\n",
    "\n",
    "# Attach diagram_id\n",
    "human_eval_with_ids = (\n",
    "    human_eval_df\n",
    "    .merge(original_dataset[[\"diagram_id\", \"prompt\", \"tikz\"]], on=[\"prompt\", \"tikz\"], how=\"inner\")\n",
    "    .drop_duplicates(\"diagram_id\", keep=\"first\")\n",
    ")\n",
    "\n",
    "# Human column -> backtranslation output column\n",
    "column_mapping = {\n",
    "    \"Diagram is fully in frame\": \"diagram_fully_in_canvas_passed\",\n",
    "    \"Diagram elements are scaled to be readable\": \"diagram_elements_are_readable_size_passed\",\n",
    "    \"Diagram elements don't problematically overlap\": \"diagram_elements_dont_problematically_overlap_passed\",\n",
    "    \"Labeled angles (if any) match drawn angle\": \"angle_labels_matches_arcs_passed\",\n",
    "    \"Labeled lengths and areas (if any) match visual proportions\": \"labeled_lengths_areas_match_proportions_passed\",\n",
    "    \"Labels (if any) are associated with correct elements\": \"labels_associated_with_elements_passed\",\n",
    "}\n",
    "\n",
    "print(f\"Human eval rows (raw): {len(human_eval_raw)}\")\n",
    "print(f\"Human eval rows matched to dataset: {len(human_eval_with_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the distrubition of human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yes (#)</th>\n",
       "      <th>Yes (%)</th>\n",
       "      <th>No (#)</th>\n",
       "      <th>No (%)</th>\n",
       "      <th>N/A (#)</th>\n",
       "      <th>N/A (%)</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shapes closed</th>\n",
       "      <td>378</td>\n",
       "      <td>0.979</td>\n",
       "      <td>8</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Core math correct</th>\n",
       "      <td>366</td>\n",
       "      <td>0.948</td>\n",
       "      <td>20</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diagram fully in canvas</th>\n",
       "      <td>345</td>\n",
       "      <td>0.894</td>\n",
       "      <td>41</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elements are readable size</th>\n",
       "      <td>369</td>\n",
       "      <td>0.956</td>\n",
       "      <td>17</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elements do not problematically overlap</th>\n",
       "      <td>297</td>\n",
       "      <td>0.769</td>\n",
       "      <td>89</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Angle labels match arcs</th>\n",
       "      <td>36</td>\n",
       "      <td>0.093</td>\n",
       "      <td>10</td>\n",
       "      <td>0.026</td>\n",
       "      <td>340</td>\n",
       "      <td>0.881</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labeled lengths/areas match proportions</th>\n",
       "      <td>91</td>\n",
       "      <td>0.236</td>\n",
       "      <td>50</td>\n",
       "      <td>0.130</td>\n",
       "      <td>245</td>\n",
       "      <td>0.635</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Labels associated with elements</th>\n",
       "      <td>191</td>\n",
       "      <td>0.495</td>\n",
       "      <td>40</td>\n",
       "      <td>0.104</td>\n",
       "      <td>155</td>\n",
       "      <td>0.402</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Yes (#)  Yes (%)  No (#)  No (%)  \\\n",
       "Rubric                                                                      \n",
       "Shapes closed                                378    0.979       8   0.021   \n",
       "Core math correct                            366    0.948      20   0.052   \n",
       "Diagram fully in canvas                      345    0.894      41   0.106   \n",
       "Elements are readable size                   369    0.956      17   0.044   \n",
       "Elements do not problematically overlap      297    0.769      89   0.231   \n",
       "Angle labels match arcs                       36    0.093      10   0.026   \n",
       "Labeled lengths/areas match proportions       91    0.236      50   0.130   \n",
       "Labels associated with elements              191    0.495      40   0.104   \n",
       "\n",
       "                                         N/A (#)  N/A (%)  Total  \n",
       "Rubric                                                            \n",
       "Shapes closed                                  0    0.000    386  \n",
       "Core math correct                              0    0.000    386  \n",
       "Diagram fully in canvas                        0    0.000    386  \n",
       "Elements are readable size                     0    0.000    386  \n",
       "Elements do not problematically overlap        0    0.000    386  \n",
       "Angle labels match arcs                      340    0.881    386  \n",
       "Labeled lengths/areas match proportions      245    0.635    386  \n",
       "Labels associated with elements              155    0.402    386  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ordered_rubrics = [\n",
    "    (\"Shape(s) is closed\", \"Shapes closed\"),\n",
    "    (\"Core math properties of the shape(s) are correct\", \"Core math correct\"),\n",
    "    (\"Diagram is fully in frame\", \"Diagram fully in canvas\"),\n",
    "    (\"Diagram elements are scaled to be readable\", \"Elements are readable size\"),\n",
    "    (\"Diagram elements don't problematically overlap\", \"Elements do not problematically overlap\"),\n",
    "    (\"Labeled angles (if any) match drawn angle\", \"Angle labels match arcs\"),\n",
    "    (\"Labeled lengths and areas (if any) match visual proportions\", \"Labeled lengths/areas match proportions\"),\n",
    "    (\"Labels (if any) are associated with correct elements\", \"Labels associated with elements\"),\n",
    "]\n",
    "\n",
    "summary = []\n",
    "for raw_name, pretty_name in ordered_rubrics:\n",
    "    counts = human_eval_df[raw_name].astype(str).str.strip().value_counts()\n",
    "    total = int(counts.sum())\n",
    "    yes = int(counts.get(\"Yes\", 0))\n",
    "    no = int(counts.get(\"No\", 0))\n",
    "    na = int(counts.get(\"N/A\", 0))\n",
    "\n",
    "    summary.append({\n",
    "        \"Rubric\": pretty_name,\n",
    "        \"Yes (#)\": yes,\n",
    "        \"Yes (%)\": yes / total if total else 0,\n",
    "        \"No (#)\": no,\n",
    "        \"No (%)\": no / total if total else 0,\n",
    "        \"N/A (#)\": na,\n",
    "        \"N/A (%)\": na / total if total else 0,\n",
    "        \"Total\": total,\n",
    "    })\n",
    "\n",
    "human_error_stats = pd.DataFrame(summary).set_index(\"Rubric\").round({\n",
    "    \"Yes (%)\": 3, \"No (%)\": 3, \"N/A (%)\": 3\n",
    "})\n",
    "\n",
    "display(human_error_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate human agreement (Cohen's Kappa) for Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5: avg_kappa=0.556\n",
      "gpt-5-mini: avg_kappa=0.527\n",
      "gpt-4.1: avg_kappa=0.563\n",
      "gpt-4.1-mini: avg_kappa=0.499\n"
     ]
    }
   ],
   "source": [
    "all_kappa_results = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    if model not in dfs:\n",
    "        continue\n",
    "    merged = human_eval_with_ids.merge(dfs[model], on=\"diagram_id\", how=\"inner\")\n",
    "    model_results = {}\n",
    "\n",
    "    for human_col, auto_col in column_mapping.items():\n",
    "        human_vals = merged[human_col].astype(str).str.strip().str.lower().map(label_map).fillna(\"N/A\")\n",
    "        auto_vals = merged[auto_col].astype(str).str.strip().str.lower().map(label_map).fillna(\"N/A\")\n",
    "\n",
    "        model_results[human_col] = {\n",
    "            \"kappa\": cohen_kappa_score(human_vals, auto_vals),\n",
    "            \"n_examples\": len(merged),\n",
    "            \"human_distribution\": human_vals.value_counts().to_dict(),\n",
    "            \"auto_distribution\": auto_vals.value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "    all_kappa_results[model] = model_results\n",
    "    print(f\"{model}: avg_kappa={sum(r['kappa'] for r in model_results.values()) / len(model_results):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can move onto calculating human agreement for LLM-as-a-judge \n",
    "\n",
    "### Load latest LLM-as-Judge CSVs (one per mode/model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded judge data: modes=['both'], models=['gpt-4.1', 'gpt-4.1-mini', 'gpt-5', 'gpt-5-mini']\n"
     ]
    }
   ],
   "source": [
    "# Load latest LLM-as-Judge CSVs (inline, no helpers)\n",
    "judge_results_root = REPO_ROOT / \"results\" / \"llm_judge\"\n",
    "pattern = re.compile(r\"evaluation_results_(?P<mode>[^_]+)_(?P<model>.+)_(?P<date>\\d{8})\\.csv$\")\n",
    "latest = {}\n",
    "\n",
    "for path in sorted(judge_results_root.glob(\"evaluation_results_*.csv\")):\n",
    "    match = pattern.match(path.name)\n",
    "    if not match:\n",
    "        continue\n",
    "    mode = match.group(\"mode\")\n",
    "    model = match.group(\"model\")\n",
    "    date = match.group(\"date\")\n",
    "    key = (mode, model)\n",
    "    if key not in latest or date > latest[key][0]:\n",
    "        latest[key] = (date, path)\n",
    "\n",
    "if not latest:\n",
    "    raise RuntimeError(f\"No judge CSV summaries found under {judge_results_root}\")\n",
    "\n",
    "judge_frames = []\n",
    "for (mode, model), (_, path) in sorted(latest.items()):\n",
    "    frame = pd.read_csv(path)\n",
    "    if frame.empty:\n",
    "        continue\n",
    "    if \"mode\" not in frame.columns:\n",
    "        frame[\"mode\"] = mode\n",
    "    if \"model\" not in frame.columns:\n",
    "        frame[\"model\"] = model\n",
    "    if \"cost\" not in frame.columns:\n",
    "        frame[\"cost\"] = 0.0\n",
    "    judge_frames.append(frame)\n",
    "\n",
    "if not judge_frames:\n",
    "    raise RuntimeError(f\"Judge CSV summaries were found but empty under {judge_results_root}\")\n",
    "\n",
    "judge_df = pd.concat(judge_frames, ignore_index=True)\n",
    "judge_df[\"diagram_id\"] = judge_df[\"diagram_id\"].astype(str)\n",
    "judge_df = judge_df.drop_duplicates([\"mode\", \"model\", \"diagram_id\"], keep=\"first\")\n",
    "judge_modes = sorted(judge_df[\"mode\"].unique())\n",
    "print(f\"Loaded judge data: modes={judge_modes}, models={sorted(judge_df['model'].unique())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both/gpt-5: avg_kappa=0.498\n",
      "both/gpt-5-mini: avg_kappa=0.465\n",
      "both/gpt-4.1: avg_kappa=0.399\n",
      "both/gpt-4.1-mini: avg_kappa=0.388\n"
     ]
    }
   ],
   "source": [
    "judge_column_mapping = {\n",
    "    \"Diagram is fully in frame\": \"diagram_fully_in_canvas_value\",\n",
    "    \"Diagram elements are scaled to be readable\": \"diagram_elements_are_readable_size_value\",\n",
    "    \"Diagram elements don't problematically overlap\": \"diagram_elements_dont_problematically_overlap_value\",\n",
    "    \"Labeled angles (if any) match drawn angle\": \"angle_labels_matches_arcs_value\",\n",
    "    \"Labeled lengths and areas (if any) match visual proportions\": \"labeled_lengths_areas_match_proportions_value\",\n",
    "    \"Labels (if any) are associated with correct elements\": \"labels_associated_with_elements_value\",\n",
    "}\n",
    "\n",
    "judge_modes = sorted(judge_df[\"mode\"].unique())\n",
    "\n",
    "human_for_judge = human_eval_with_ids.copy()\n",
    "human_for_judge[\"diagram_id\"] = human_for_judge[\"diagram_id\"].astype(str)\n",
    "human_for_judge = human_for_judge.drop_duplicates(\"diagram_id\", keep=\"first\")\n",
    "\n",
    "all_judge_kappa_results = {}\n",
    "\n",
    "for mode in judge_modes:\n",
    "    mode_results = {}\n",
    "    for model in MODELS:\n",
    "        model_df = judge_df[(judge_df[\"mode\"] == mode) & (judge_df[\"model\"] == model)].copy()\n",
    "        if model_df.empty:\n",
    "            continue\n",
    "\n",
    "        model_df[\"diagram_id\"] = model_df[\"diagram_id\"].astype(str)\n",
    "        model_df = model_df.drop_duplicates(\"diagram_id\", keep=\"first\")\n",
    "        merged = human_for_judge.merge(model_df, on=\"diagram_id\", how=\"inner\")\n",
    "\n",
    "        rubric_results = {}\n",
    "        for human_col, judge_col in judge_column_mapping.items():\n",
    "            human_vals = merged[human_col].astype(str).str.strip().str.lower().map(label_map).fillna(\"N/A\")\n",
    "            judge_vals = merged[judge_col].astype(str).str.strip().str.lower().map(label_map).fillna(\"N/A\")\n",
    "            rubric_results[human_col] = {\n",
    "                \"kappa\": cohen_kappa_score(human_vals, judge_vals),\n",
    "                \"n_examples\": len(merged),\n",
    "                \"human_distribution\": human_vals.value_counts().to_dict(),\n",
    "                \"judge_distribution\": judge_vals.value_counts().to_dict(),\n",
    "            }\n",
    "\n",
    "        mode_results[model] = rubric_results\n",
    "        print(f\"{mode}/{model}: avg_kappa={sum(r['kappa'] for r in rubric_results.values()) / len(rubric_results):.3f}\")\n",
    "\n",
    "    all_judge_kappa_results[mode] = mode_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "κ comparisons (Backtranslation vs LLM-as-a-Judge modes):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_50f84_row0_col7, #T_50f84_row1_col6, #T_50f84_row2_col2, #T_50f84_row3_col0, #T_50f84_row4_col0, #T_50f84_row5_col2 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_50f84\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_50f84_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">Backtranslation</th>\n",
       "      <th id=\"T_50f84_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">Judge – both</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_50f84_level1_col0\" class=\"col_heading level1 col0\" >gpt 4.1</th>\n",
       "      <th id=\"T_50f84_level1_col1\" class=\"col_heading level1 col1\" >gpt 4.1 mini</th>\n",
       "      <th id=\"T_50f84_level1_col2\" class=\"col_heading level1 col2\" >gpt 5</th>\n",
       "      <th id=\"T_50f84_level1_col3\" class=\"col_heading level1 col3\" >gpt 5 mini</th>\n",
       "      <th id=\"T_50f84_level1_col4\" class=\"col_heading level1 col4\" >gpt 4.1</th>\n",
       "      <th id=\"T_50f84_level1_col5\" class=\"col_heading level1 col5\" >gpt 4.1 mini</th>\n",
       "      <th id=\"T_50f84_level1_col6\" class=\"col_heading level1 col6\" >gpt 5</th>\n",
       "      <th id=\"T_50f84_level1_col7\" class=\"col_heading level1 col7\" >gpt 5 mini</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >rubric</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row0\" class=\"row_heading level0 row0\" >Labeled angles (if any) match drawn angle</th>\n",
       "      <td id=\"T_50f84_row0_col0\" class=\"data row0 col0\" >0.691000</td>\n",
       "      <td id=\"T_50f84_row0_col1\" class=\"data row0 col1\" >0.593000</td>\n",
       "      <td id=\"T_50f84_row0_col2\" class=\"data row0 col2\" >0.644000</td>\n",
       "      <td id=\"T_50f84_row0_col3\" class=\"data row0 col3\" >0.652000</td>\n",
       "      <td id=\"T_50f84_row0_col4\" class=\"data row0 col4\" >0.793000</td>\n",
       "      <td id=\"T_50f84_row0_col5\" class=\"data row0 col5\" >0.791000</td>\n",
       "      <td id=\"T_50f84_row0_col6\" class=\"data row0 col6\" >0.795000</td>\n",
       "      <td id=\"T_50f84_row0_col7\" class=\"data row0 col7\" >0.829000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row1\" class=\"row_heading level0 row1\" >Labeled lengths and areas (if any) match visual proportions</th>\n",
       "      <td id=\"T_50f84_row1_col0\" class=\"data row1 col0\" >0.449000</td>\n",
       "      <td id=\"T_50f84_row1_col1\" class=\"data row1 col1\" >0.429000</td>\n",
       "      <td id=\"T_50f84_row1_col2\" class=\"data row1 col2\" >0.429000</td>\n",
       "      <td id=\"T_50f84_row1_col3\" class=\"data row1 col3\" >0.422000</td>\n",
       "      <td id=\"T_50f84_row1_col4\" class=\"data row1 col4\" >0.581000</td>\n",
       "      <td id=\"T_50f84_row1_col5\" class=\"data row1 col5\" >0.596000</td>\n",
       "      <td id=\"T_50f84_row1_col6\" class=\"data row1 col6\" >0.673000</td>\n",
       "      <td id=\"T_50f84_row1_col7\" class=\"data row1 col7\" >0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row2\" class=\"row_heading level0 row2\" >Diagram is fully in frame</th>\n",
       "      <td id=\"T_50f84_row2_col0\" class=\"data row2 col0\" >0.573000</td>\n",
       "      <td id=\"T_50f84_row2_col1\" class=\"data row2 col1\" >0.541000</td>\n",
       "      <td id=\"T_50f84_row2_col2\" class=\"data row2 col2\" >0.604000</td>\n",
       "      <td id=\"T_50f84_row2_col3\" class=\"data row2 col3\" >0.581000</td>\n",
       "      <td id=\"T_50f84_row2_col4\" class=\"data row2 col4\" >0.184000</td>\n",
       "      <td id=\"T_50f84_row2_col5\" class=\"data row2 col5\" >0.162000</td>\n",
       "      <td id=\"T_50f84_row2_col6\" class=\"data row2 col6\" >0.390000</td>\n",
       "      <td id=\"T_50f84_row2_col7\" class=\"data row2 col7\" >0.398000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row3\" class=\"row_heading level0 row3\" >Diagram elements are scaled to be readable</th>\n",
       "      <td id=\"T_50f84_row3_col0\" class=\"data row3 col0\" >0.362000</td>\n",
       "      <td id=\"T_50f84_row3_col1\" class=\"data row3 col1\" >0.308000</td>\n",
       "      <td id=\"T_50f84_row3_col2\" class=\"data row3 col2\" >0.334000</td>\n",
       "      <td id=\"T_50f84_row3_col3\" class=\"data row3 col3\" >0.272000</td>\n",
       "      <td id=\"T_50f84_row3_col4\" class=\"data row3 col4\" >-0.017000</td>\n",
       "      <td id=\"T_50f84_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_50f84_row3_col6\" class=\"data row3 col6\" >0.043000</td>\n",
       "      <td id=\"T_50f84_row3_col7\" class=\"data row3 col7\" >0.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row4\" class=\"row_heading level0 row4\" >Labels (if any) are associated with correct elements</th>\n",
       "      <td id=\"T_50f84_row4_col0\" class=\"data row4 col0\" >0.812000</td>\n",
       "      <td id=\"T_50f84_row4_col1\" class=\"data row4 col1\" >0.688000</td>\n",
       "      <td id=\"T_50f84_row4_col2\" class=\"data row4 col2\" >0.715000</td>\n",
       "      <td id=\"T_50f84_row4_col3\" class=\"data row4 col3\" >0.687000</td>\n",
       "      <td id=\"T_50f84_row4_col4\" class=\"data row4 col4\" >0.789000</td>\n",
       "      <td id=\"T_50f84_row4_col5\" class=\"data row4 col5\" >0.780000</td>\n",
       "      <td id=\"T_50f84_row4_col6\" class=\"data row4 col6\" >0.768000</td>\n",
       "      <td id=\"T_50f84_row4_col7\" class=\"data row4 col7\" >0.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_50f84_level0_row5\" class=\"row_heading level0 row5\" >Diagram elements don't problematically overlap</th>\n",
       "      <td id=\"T_50f84_row5_col0\" class=\"data row5 col0\" >0.489000</td>\n",
       "      <td id=\"T_50f84_row5_col1\" class=\"data row5 col1\" >0.436000</td>\n",
       "      <td id=\"T_50f84_row5_col2\" class=\"data row5 col2\" >0.608000</td>\n",
       "      <td id=\"T_50f84_row5_col3\" class=\"data row5 col3\" >0.549000</td>\n",
       "      <td id=\"T_50f84_row5_col4\" class=\"data row5 col4\" >0.062000</td>\n",
       "      <td id=\"T_50f84_row5_col5\" class=\"data row5 col5\" >0.000000</td>\n",
       "      <td id=\"T_50f84_row5_col6\" class=\"data row5 col6\" >0.315000</td>\n",
       "      <td id=\"T_50f84_row5_col7\" class=\"data row5 col7\" >0.094000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12dba1730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define rubrics to compare and model name mappings\n",
    "rubrics = [\n",
    "    \"Labeled angles (if any) match drawn angle\",\n",
    "    \"Labeled lengths and areas (if any) match visual proportions\",\n",
    "    \"Diagram is fully in frame\",\n",
    "    \"Diagram elements are scaled to be readable\",\n",
    "    \"Labels (if any) are associated with correct elements\",\n",
    "    \"Diagram elements don't problematically overlap\",\n",
    "]\n",
    "\n",
    "friendly = {\n",
    "    \"gpt-4.1\": \"gpt 4.1\",\n",
    "    \"gpt-4.1-mini\": \"gpt 4.1 mini\",\n",
    "    \"gpt-5\": \"gpt 5\",\n",
    "    \"gpt-5-mini\": \"gpt 5 mini\",\n",
    "}\n",
    "\n",
    "models = list(friendly.keys())  # Avoid repeating model list\n",
    "\n",
    "# Build comparison table: each row is a rubric, columns are (method, model) pairs\n",
    "summary_rows = []\n",
    "for rubric in rubrics:\n",
    "    row = {\"rubric\": rubric}\n",
    "    \n",
    "    # Add Backtranslation kappa values for each model\n",
    "    for model in models:\n",
    "        kappa_val = all_kappa_results.get(model, {}).get(rubric, {}).get(\"kappa\")\n",
    "        row[(\"Backtranslation\", friendly[model])] = kappa_val\n",
    "    \n",
    "    # Add Judge kappa values for each mode and model\n",
    "    for mode in sorted(judge_df[\"mode\"].unique()):\n",
    "        mode_results = all_judge_kappa_results.get(mode, {})\n",
    "        for model in models:\n",
    "            kappa_val = mode_results.get(model, {}).get(rubric, {}).get(\"kappa\")\n",
    "            row[(f\"Judge – {mode}\", friendly[model])] = kappa_val\n",
    "    \n",
    "    summary_rows.append(row)\n",
    "\n",
    "# Create DataFrame with multi-level columns: (method, model)\n",
    "kappa_table_only = pd.DataFrame(summary_rows).set_index(\"rubric\")\n",
    "kappa_table_only.columns = pd.MultiIndex.from_tuples(kappa_table_only.columns)\n",
    "\n",
    "# Display table with max values per row highlighted in bold\n",
    "print(\"\\nκ comparisons (Backtranslation vs LLM-as-a-Judge modes):\")\n",
    "styled_table = kappa_table_only.round(3).style.highlight_max(axis=1, props=\"font-weight: bold;\")\n",
    "display(styled_table)\n",
    "\n",
    "# Store column references for later use in average kappa calculations\n",
    "cols_back = [(\"Backtranslation\", name) for name in friendly.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtranslation summary\n",
    "bt_summary_rows = []\n",
    "for MODEL in MODELS:\n",
    "    if MODEL not in all_kappa_results:\n",
    "        continue\n",
    "    kappa_results = all_kappa_results[MODEL]\n",
    "    kappas = [res['kappa'] for res in kappa_results.values() if res.get('kappa') is not None]\n",
    "    avg_kappa = sum(kappas) / len(kappas) if kappas else float('nan')\n",
    "    bt_summary_rows.append({\n",
    "        'model': friendly[MODEL], \n",
    "        'cohen_k': avg_kappa,\n",
    "    })\n",
    "bt_summary_df = pd.DataFrame(bt_summary_rows)\n",
    "\n",
    "# Judge summary (all modes)\n",
    "judge_summary_rows = []\n",
    "for mode in judge_modes:\n",
    "    for MODEL in MODELS:\n",
    "        kappa_results = all_judge_kappa_results.get(mode, {}).get(MODEL)\n",
    "        if not kappa_results:\n",
    "            continue\n",
    "        kappas = [res[\"kappa\"] for res in kappa_results.values() if res.get(\"kappa\") is not None]\n",
    "        avg_kappa = sum(kappas) / len(kappas) if kappas else float(\"nan\")\n",
    "\n",
    "        judge_summary_rows.append({\n",
    "            \"mode\": mode,\n",
    "            \"model\": friendly[MODEL], \n",
    "            \"cohen_k\": avg_kappa,\n",
    "        })\n",
    "judge_summary_df = pd.DataFrame(judge_summary_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtranslation vs LLM-as-a-Judge (both)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BT κ</th>\n",
       "      <th>BT time (s)</th>\n",
       "      <th>BT cost ($)</th>\n",
       "      <th>Judge (both) κ</th>\n",
       "      <th>Judge (both) time (s)</th>\n",
       "      <th>Judge (both) cost ($)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt 4.1</th>\n",
       "      <td>0.563</td>\n",
       "      <td>25.74</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.399</td>\n",
       "      <td>14.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt 5</th>\n",
       "      <td>0.556</td>\n",
       "      <td>36.93</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.498</td>\n",
       "      <td>26.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt 4.1 mini</th>\n",
       "      <td>0.499</td>\n",
       "      <td>24.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.388</td>\n",
       "      <td>8.55</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt 5 mini</th>\n",
       "      <td>0.527</td>\n",
       "      <td>42.26</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.465</td>\n",
       "      <td>12.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BT κ  BT time (s)  BT cost ($)  Judge (both) κ  \\\n",
       "Model                                                           \n",
       "gpt 4.1       0.563        25.74         6.76           0.399   \n",
       "gpt 5         0.556        36.93        10.30           0.498   \n",
       "gpt 4.1 mini  0.499        24.69         0.00           0.388   \n",
       "gpt 5 mini    0.527        42.26         2.25           0.465   \n",
       "\n",
       "              Judge (both) time (s)  Judge (both) cost ($)  \n",
       "Model                                                       \n",
       "gpt 4.1                       14.50                    0.0  \n",
       "gpt 5                         26.20                    0.0  \n",
       "gpt 4.1 mini                   8.55                    0.0  \n",
       "gpt 5 mini                    12.66                    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_pipeline_stats(*, df, avg_kappa, time_col_ms, cost_col):\n",
    "    avg_time_s = pd.to_numeric(df[time_col_ms], errors='coerce').fillna(0).mean() / 1000\n",
    "    # Handle missing cost column (backtranslation CSVs don't have cost column)\n",
    "    if cost_col in df.columns:\n",
    "        total_cost = pd.to_numeric(df[cost_col], errors='coerce').fillna(0).sum()\n",
    "    else:\n",
    "        total_cost = 0.0\n",
    "    return avg_kappa, avg_time_s, total_cost\n",
    "\n",
    "comparison_tables = {}\n",
    "\n",
    "for mode in sorted(judge_df[\"mode\"].unique()):\n",
    "    rows = []\n",
    "    for model_id in [\"gpt-4.1\", \"gpt-5\", \"gpt-4.1-mini\", \"gpt-5-mini\"]:\n",
    "        bt_df = dfs[model_id].copy()\n",
    "        bt_df[\"total_time_ms\"] = (\n",
    "            pd.to_numeric(bt_df[\"extraction_time_ms\"], errors='coerce').fillna(0)\n",
    "            + pd.to_numeric(bt_df[\"evaluation_time_ms\"], errors='coerce').fillna(0)\n",
    "        )\n",
    "        bt_avg_kappa = bt_summary_df.set_index(\"model\").loc[friendly[model_id], \"cohen_k\"]\n",
    "        bt_k, bt_time, bt_cost = compute_pipeline_stats(\n",
    "            df=bt_df,\n",
    "            avg_kappa=bt_avg_kappa,\n",
    "            time_col_ms=\"total_time_ms\",\n",
    "            cost_col=\"extraction_cost\",\n",
    "        )\n",
    "\n",
    "        judge_model_df = judge_df[(judge_df[\"mode\"] == mode) & (judge_df[\"model\"] == model_id)]\n",
    "        judge_avg_kappa = judge_summary_df.set_index([\"mode\", \"model\"]).loc[(mode, friendly[model_id]), \"cohen_k\"]\n",
    "        judge_k, judge_time, judge_cost = compute_pipeline_stats(\n",
    "            df=judge_model_df,\n",
    "            avg_kappa=judge_avg_kappa,\n",
    "            time_col_ms=\"elapsed_ms\",\n",
    "            cost_col=\"cost\",\n",
    "        )\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": friendly[model_id],\n",
    "            \"BT κ\": bt_k,\n",
    "            \"BT time (s)\": bt_time,\n",
    "            \"BT cost ($)\": bt_cost,\n",
    "            f\"Judge ({mode}) κ\": judge_k,\n",
    "            f\"Judge ({mode}) time (s)\": judge_time,\n",
    "            f\"Judge ({mode}) cost ($)\": judge_cost,\n",
    "        })\n",
    "\n",
    "    comparison_tables[mode] = (\n",
    "        pd.DataFrame(rows)\n",
    "        .set_index(\"Model\")\n",
    "        .round({\"BT κ\": 3, \"BT time (s)\": 2, \"BT cost ($)\": 2,\n",
    "                f\"Judge ({mode}) κ\": 3, f\"Judge ({mode}) time (s)\": 2, f\"Judge ({mode}) cost ($)\": 2})\n",
    "    )\n",
    "\n",
    "for mode, table in comparison_tables.items():\n",
    "    print(f\"\\nBacktranslation vs LLM-as-a-Judge ({mode})\")\n",
    "    display(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'extraction_cost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/code/DiagramIR/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'extraction_cost'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m CACHE_DISCOUNT = \u001b[32m0.25\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Backtranslation: use recorded extraction_cost from CSV\u001b[39;00m\n\u001b[32m     11\u001b[39m bt_cost = (\n\u001b[32m     12\u001b[39m     pd.concat([\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: [model] * \u001b[38;5;28mlen\u001b[39m(dfs[model]), \u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m: pd.to_numeric(\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mextraction_cost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m).fillna(\u001b[32m0\u001b[39m)})\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m MODELS \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m dfs\n\u001b[32m     15\u001b[39m     ], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m     .groupby(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     17\u001b[39m     .agg(n_diagrams=(\u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m), avg_cost_per_diagram_usd=(\u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m), total_cost_usd=(\u001b[33m\"\u001b[39m\u001b[33mcost\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m bt_cost.insert(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m bt_cost.insert(\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbacktranslation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/code/DiagramIR/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/code/DiagramIR/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'extraction_cost'"
     ]
    }
   ],
   "source": [
    "# Cost summary (simple)\n",
    "MODEL_COSTS = {\n",
    "    \"gpt-5\": (1.25, 10.00),\n",
    "    \"gpt-5-mini\": (0.25, 2.00),\n",
    "    \"gpt-4.1\": (2.00, 8.00),\n",
    "    \"gpt-4.1-mini\": (0.40, 1.60),\n",
    "}\n",
    "CACHE_DISCOUNT = 0.25\n",
    "\n",
    "# Backtranslation: use recorded extraction_cost from CSV\n",
    "bt_cost = (\n",
    "    pd.concat([\n",
    "        pd.DataFrame({\"model\": [model] * len(dfs[model]), \"cost\": pd.to_numeric(dfs[model][\"extraction_cost\"], errors=\"coerce\").fillna(0)})\n",
    "        for model in MODELS if model in dfs\n",
    "    ], ignore_index=True)\n",
    "    .groupby(\"model\", as_index=False)\n",
    "    .agg(n_diagrams=(\"cost\", \"size\"), avg_cost_per_diagram_usd=(\"cost\", \"mean\"), total_cost_usd=(\"cost\", \"sum\"))\n",
    ")\n",
    "bt_cost.insert(0, \"mode\", \"-\")\n",
    "bt_cost.insert(0, \"pipeline\", \"backtranslation\")\n",
    "\n",
    "# Judge: recompute cost from token counts in CSV\n",
    "judge_tmp = judge_df.copy()\n",
    "judge_tmp[\"input_tokens\"] = pd.to_numeric(judge_tmp.get(\"input_tokens\", 0), errors=\"coerce\").fillna(0)\n",
    "judge_tmp[\"cached_tokens\"] = pd.to_numeric(judge_tmp.get(\"cached_tokens\", 0), errors=\"coerce\").fillna(0)\n",
    "judge_tmp[\"output_tokens\"] = pd.to_numeric(judge_tmp.get(\"output_tokens\", 0), errors=\"coerce\").fillna(0)\n",
    "\n",
    "def row_cost(row):\n",
    "    prompt_per_m, completion_per_m = MODEL_COSTS[row[\"model\"]]\n",
    "    prompt_rate = prompt_per_m / 1_000_000\n",
    "    completion_rate = completion_per_m / 1_000_000\n",
    "    cached_rate = prompt_rate * CACHE_DISCOUNT\n",
    "\n",
    "    input_tokens = row[\"input_tokens\"]\n",
    "    cached_tokens = min(row[\"cached_tokens\"], input_tokens)\n",
    "    fresh_tokens = input_tokens - cached_tokens\n",
    "    output_tokens = row[\"output_tokens\"]\n",
    "\n",
    "    return fresh_tokens * prompt_rate + cached_tokens * cached_rate + output_tokens * completion_rate\n",
    "\n",
    "judge_tmp[\"cost\"] = judge_tmp.apply(row_cost, axis=1)\n",
    "judge_cost = (\n",
    "    judge_tmp.groupby([\"mode\", \"model\"], as_index=False)\n",
    "    .agg(n_diagrams=(\"cost\", \"size\"), avg_cost_per_diagram_usd=(\"cost\", \"mean\"), total_cost_usd=(\"cost\", \"sum\"))\n",
    ")\n",
    "judge_cost.insert(0, \"pipeline\", \"llm_judge\")\n",
    "\n",
    "cost_summary = pd.concat([bt_cost, judge_cost], ignore_index=True)\n",
    "cost_summary = cost_summary.sort_values([\"pipeline\", \"mode\", \"model\"]).reset_index(drop=True)\n",
    "display(cost_summary.round({\"avg_cost_per_diagram_usd\": 6, \"total_cost_usd\": 4}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
