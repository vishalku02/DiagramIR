{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates our backtranslation method:\n",
    "1. LLM extraction of TikZ \u2192 IR \n",
    "2. Rule-based evaluation checks on IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vishalkumar/Desktop/Research/code/DiagramIR/backtranslation.py:19: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  5. For rectangle_primitives array, set \"is_right_angle_symbol\": true when the rectangle is drawn as a right-angle marker (e.g., tiny square sharing corners with two incident edges or comments mentioning a right angle). Otherwise set it to false. If a \\draw explicitly passes the 'right angle symbol' option, do not add it as a rectangle_primitive.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: gpt-4.1-mini\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, pathlib, os\n",
    "\n",
    "notebook_dir = pathlib.Path.cwd()\n",
    "target = (notebook_dir / \"..\").resolve()\n",
    "os.chdir(target)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import aiofiles\n",
    "from pathlib import Path\n",
    "from tqdm.asyncio import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from backtranslation import tikz_to_ir\n",
    "from evaluator import Evaluator\n",
    "from utils.ir_schema import TikzIR\n",
    "\n",
    "# Get model from environment\n",
    "MODEL = os.environ.get(\"OPENAI_MODEL\")\n",
    "print(f\"Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: results/backtranslation\n",
      "Cache directory: results/backtranslation/cache\n"
     ]
    }
   ],
   "source": [
    "RESULTS_DIR = Path(\"results/backtranslation\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CACHE_DIR = RESULTS_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Rate limiting\n",
    "MAX_CONCURRENT = 6  # Adjust based on OpenAI rate limits\n",
    "REQUEST_DELAY = 0.075  # delay between requests in seconds\n",
    "\n",
    "TEST_SET_PATH = \"data/geometric_shapes_test_set.csv\"\n",
    "\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Loaded 398 examples\n",
      "Columns: ['prompt', 'tikz', 'image', 'main_category', 'subcategory', 'diagram_id', 'assignment_type', 'assigned_to', 'image_png_path']\n",
      "Categories: main_category\n",
      "2d shapes    208\n",
      "3d shapes    190\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>tikz</th>\n",
       "      <th>image</th>\n",
       "      <th>main_category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>diagram_id</th>\n",
       "      <th>assignment_type</th>\n",
       "      <th>assigned_to</th>\n",
       "      <th>image_png_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triangle with side length 8 horizontal at bott...</td>\n",
       "      <td>\\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...</td>\n",
       "      <td>https://2xavun1dsa0sayar.public.blob.vercel-st...</td>\n",
       "      <td>2d shapes</td>\n",
       "      <td>triangle</td>\n",
       "      <td>1</td>\n",
       "      <td>individual</td>\n",
       "      <td>Shubhra</td>\n",
       "      <td>data/judge_pngs/diagram_1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two triangles showing scaled copy relationship...</td>\n",
       "      <td>\\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...</td>\n",
       "      <td>https://2xavun1dsa0sayar.public.blob.vercel-st...</td>\n",
       "      <td>2d shapes</td>\n",
       "      <td>triangle</td>\n",
       "      <td>2</td>\n",
       "      <td>individual</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>data/judge_pngs/diagram_2.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  triangle with side length 8 horizontal at bott...   \n",
       "1  Two triangles showing scaled copy relationship...   \n",
       "\n",
       "                                                tikz  \\\n",
       "0  \\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...   \n",
       "1  \\documentclass{IM}\\n\\usepackage{tikz}\\n\\begin{...   \n",
       "\n",
       "                                               image main_category  \\\n",
       "0  https://2xavun1dsa0sayar.public.blob.vercel-st...     2d shapes   \n",
       "1  https://2xavun1dsa0sayar.public.blob.vercel-st...     2d shapes   \n",
       "\n",
       "  subcategory  diagram_id assignment_type assigned_to  \\\n",
       "0    triangle           1      individual     Shubhra   \n",
       "1    triangle           2      individual     Rebecca   \n",
       "\n",
       "                  image_png_path  \n",
       "0  data/judge_pngs/diagram_1.png  \n",
       "1  data/judge_pngs/diagram_2.png  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading test dataset...\")\n",
    "df = pd.read_csv(TEST_SET_PATH)\n",
    "print(f\"Loaded {len(df)} examples\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Categories: {df['main_category'].value_counts()}\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator initialized\n",
      "Global checks: ['angle_labels_matches_arcs', 'diagram_fully_in_canvas', 'labels_associated_with_elements', 'diagram_elements_dont_problematically_overlap', 'diagram_elements_are_readable_size', 'shape_outlines_are_closed', 'core_mathematical_properties_of_shapes_correct', 'labeled_lengths_areas_match_proportions']\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "print(\"Evaluator initialized\")\n",
    "print(f\"Global checks: {[fn.__name__ for fn in evaluator.global_evals]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import warnings\n",
    "\n",
    "async def get_cached_result(diagram_id: int, model: str = None) -> Optional[dict]:\n",
    "    \"\"\"Check if result is already cached\"\"\"\n",
    "    if model is None:\n",
    "        model = MODEL  # Use the global MODEL variable\n",
    "    cache_key = f\"diagram_{diagram_id}_{model}\"\n",
    "    cache_file = CACHE_DIR / f\"{cache_key}.json\"\n",
    "    if cache_file.exists():\n",
    "        async with aiofiles.open(cache_file, 'r') as f:\n",
    "            content = await f.read()\n",
    "        record = json.loads(content)\n",
    "        apply_cost_estimate(record, model)\n",
    "        return record\n",
    "    return None\n",
    "\n",
    "async def save_result(result: dict):\n",
    "    \"\"\"Save result to cache immediately\"\"\"\n",
    "    model = result.get('model', MODEL)  # Get model from result or use global\n",
    "    cache_key = f\"diagram_{result['diagram_id']}_{model}\"\n",
    "    cache_file = CACHE_DIR / f\"{cache_key}.json\"\n",
    "    apply_cost_estimate(result, model)\n",
    "    async with aiofiles.open(cache_file, 'w') as f:\n",
    "        await f.write(json.dumps(result, indent=2))\n",
    "\n",
    "\n",
    "MODEL_COSTS = {\n",
    "    \"gpt-5\":        (1.25, 10.00),\n",
    "    \"gpt-5-mini\":   (0.25, 2.00),\n",
    "    \"gpt-5-nano\":   (0.05, 0.40),\n",
    "    \"gpt-4.1\":      (2.00, 8.00),\n",
    "    \"gpt-4.1-mini\": (0.40, 1.60),\n",
    "    \"gpt-4.1-nano\": (0.10, 0.40),\n",
    "    \"gpt-4o\":       (5.00, 15.00),\n",
    "    \"gpt-4o-mini\":  (0.15, 0.60),\n",
    "}\n",
    "\n",
    "def estimate_cost(prompt_tokens: int, completion_tokens: int, model: str) -> float:\n",
    "    \"\"\"Estimate API cost using per-1M-token pricing\"\"\"\n",
    "    if model not in MODEL_COSTS:\n",
    "        warnings.warn(\n",
    "            f\"Unknown model '{model}' in MODEL_COSTS; assuming zero token cost.\",\n",
    "            RuntimeWarning,\n",
    "        )\n",
    "        input_cost, output_cost = (0.0, 0.0)\n",
    "    else:\n",
    "        input_cost, output_cost = MODEL_COSTS[model]\n",
    "    # Normalize per 1M tokens\n",
    "    input_cost /= 1_000_000\n",
    "    output_cost /= 1_000_000\n",
    "    return (prompt_tokens * input_cost) + (completion_tokens * output_cost)\n",
    "\n",
    "def apply_cost_estimate(cache_entry: dict, model_override: Optional[str] = None) -> float:\n",
    "    \"\"\"Ensure cached entries store a normalized cost estimate.\"\"\"\n",
    "    if not cache_entry:\n",
    "        return 0.0\n",
    "    ir_block = cache_entry.setdefault(\"ir\", {})\n",
    "    model_name = model_override or cache_entry.get(\"model\", MODEL)\n",
    "    prompt_tokens = ir_block.get(\"prompt_tokens\", 0) or 0\n",
    "    completion_tokens = ir_block.get(\"completion_tokens\", 0) or 0\n",
    "    cost = estimate_cost(prompt_tokens, completion_tokens, model_name)\n",
    "    ir_block[\"estimated_cost\"] = cost\n",
    "    cache_entry[\"extraction_cost\"] = cost\n",
    "    return cost\n",
    "\n",
    "if 'all_results' in globals():\n",
    "    for entry in all_results:\n",
    "        apply_cost_estimate(entry, entry.get(\"model\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_single_example(row: pd.Series, semaphore: asyncio.Semaphore) -> dict:\n",
    "    \"\"\"Process a single TikZ example with rate limiting\"\"\"\n",
    "    cached = await get_cached_result(row['diagram_id'], MODEL)\n",
    "    if cached:\n",
    "        return cached\n",
    "    \n",
    "    async with semaphore:\n",
    "        result = {\n",
    "            \"prompt\": row.get('prompt', ''),\n",
    "            \"tikz_code\": row['tikz'],\n",
    "            \"diagram_id\": row['diagram_id'],\n",
    "            \"model\": MODEL,\n",
    "            \"main_category\": row['main_category'],\n",
    "            \"subcategory\": row['subcategory'],\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"ir\": {},\n",
    "            \"evaluation_results\": {},\n",
    "            \"evaluation_time_ms\": 0,\n",
    "            \"overall_score\": 0.0,\n",
    "            \"overall_pass\": False\n",
    "        }\n",
    "        \n",
    "        token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n",
    "        raw_ir = None\n",
    "        tikz_time = 0.0\n",
    "        \n",
    "        try:\n",
    "            tikz_start = time.time()\n",
    "            raw_ir, token_usage = await tikz_to_ir(row['tikz'])\n",
    "            tikz_time = (time.time() - tikz_start) * 1000\n",
    "\n",
    "            prompt_tokens = token_usage.get('prompt_tokens', 0)\n",
    "            completion_tokens = token_usage.get('completion_tokens', 0)\n",
    "            estimated_cost = estimate_cost(prompt_tokens, completion_tokens, MODEL)\n",
    "            result[\"extraction_cost\"] = estimated_cost\n",
    "            \n",
    "            result[\"ir\"] = {\n",
    "                \"success\": True,\n",
    "                \"time_ms\": tikz_time,\n",
    "                \"estimated_cost\": estimated_cost,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"error\": None,\n",
    "                \"validation_error\": None,\n",
    "                \"generated_ir\": raw_ir\n",
    "            }\n",
    "\n",
    "            # Validate IR before evaluation, but continue even if strict validation fails\n",
    "            ir_for_evaluation = raw_ir\n",
    "            try:\n",
    "                ir_for_evaluation = TikzIR.model_validate(raw_ir)\n",
    "            except Exception as e:\n",
    "                validation_error = f\"IR validation failed: {str(e)}\"\n",
    "                result[\"ir\"][\"success\"] = False\n",
    "                result[\"ir\"][\"error\"] = validation_error\n",
    "                result[\"ir\"][\"validation_error\"] = validation_error\n",
    "\n",
    "            eval_start = time.time()\n",
    "            eval_results = evaluator.evaluate(ir_for_evaluation)\n",
    "            eval_time = (time.time() - eval_start) * 1000\n",
    "            \n",
    "            for check in eval_results['global']:\n",
    "                result[\"evaluation_results\"][check['check']] = {\n",
    "                    \"passed\": check['passed'],\n",
    "                    \"message\": check['message']\n",
    "                }\n",
    "            \n",
    "            result[\"evaluation_time_ms\"] = eval_time\n",
    "            result[\"overall_score\"] = eval_results['score']\n",
    "            result[\"overall_pass\"] = eval_results['overall_pass']\n",
    "            \n",
    "        except Exception as e:\n",
    "            fallback_cost = estimate_cost(\n",
    "                token_usage.get('prompt_tokens', 0),\n",
    "                token_usage.get('completion_tokens', 0),\n",
    "                MODEL,\n",
    "            ) if token_usage else 0\n",
    "            result[\"extraction_cost\"] = fallback_cost\n",
    "            result[\"ir\"] = {\n",
    "                \"success\": False,\n",
    "                \"time_ms\": tikz_time,\n",
    "                \"estimated_cost\": fallback_cost,\n",
    "                \"prompt_tokens\": token_usage.get('prompt_tokens', 0),\n",
    "                \"completion_tokens\": token_usage.get('completion_tokens', 0),\n",
    "                \"error\": str(e),\n",
    "                \"validation_error\": str(e),\n",
    "                \"generated_ir\": raw_ir\n",
    "            }\n",
    "            result[\"evaluation_results\"] = {}\n",
    "            result[\"evaluation_time_ms\"] = 0\n",
    "            result[\"overall_score\"] = 0.0\n",
    "            result[\"overall_pass\"] = False\n",
    "        \n",
    "        await save_result(result)\n",
    "        await asyncio.sleep(REQUEST_DELAY)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation_experiment(df: pd.DataFrame, max_examples: int = None):\n",
    "    \"\"\"Run the full evaluation experiment with parallel processing\"\"\"\n",
    "    \n",
    "    if max_examples:\n",
    "        df = df.head(max_examples)\n",
    "        print(f\"Running experiment on first {max_examples} examples\")\n",
    "    \n",
    "    print(f\"Processing {len(df)} examples with max {MAX_CONCURRENT} concurrent requests\")\n",
    "    \n",
    "    # Create semaphore for rate limiting\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
    "    \n",
    "    # Create tasks for all examples\n",
    "    tasks = [\n",
    "        process_single_example(row, semaphore) \n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Run with progress bar\n",
    "    results = await tqdm.gather(*tasks, desc=\"Processing examples\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test with 10 examples...\n",
      "Running experiment on first 10 examples\n",
      "Processing 10 examples with max 6 concurrent requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples:  10%|\u2588         | 1/10 [00:03<00:34,  3.78s/it]/Users/vishalkumar/Desktop/Research/code/DiagramIR/.venv/lib/python3.12/site-packages/shapely/constructive.py:246: RuntimeWarning: invalid value encountered in buffer\n",
      "  return lib.buffer(\n",
      "Processing examples: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:13<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed 10 examples\n",
      "Success rate: 90.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run experiment on a small subset first to test\n",
    "print(\"Running test with 10 examples...\")\n",
    "test_results = await run_evaluation_experiment(df, max_examples=10)\n",
    "print(f\"\\nCompleted {len(test_results)} examples\")\n",
    "print(f\"Success rate: {sum(1 for r in test_results if r['ir']['success']) / len(test_results):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full dataset \n",
    "print(\"Running full experiment...\")\n",
    "all_results = await run_evaluation_experiment(df, max_examples=None)  # Remove limit for full run\n",
    "\n",
    "print(f\"\\nExperiment complete! Processed {len(all_results)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed results to CSV\n",
    "def export_results_to_csv(results: list[dict], filepath: str):\n",
    "    \"\"\"Export results to CSV for further analysis\"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    for result in results:\n",
    "        row = {\n",
    "            'diagram_id': result['diagram_id'],\n",
    "            'main_category': result['main_category'],\n",
    "            'subcategory': result['subcategory'],\n",
    "            'assignment_type': result.get('assignment_type', 'unknown'),\n",
    "            'extraction_success': result['ir']['success'],\n",
    "            'extraction_time_ms': result['ir']['time_ms'],\n",
    "            'extraction_cost': result.get('extraction_cost', result['ir'].get('estimated_cost', 0)),\n",
    "            'evaluation_time_ms': result['evaluation_time_ms'],\n",
    "            'overall_score': result['overall_score'],\n",
    "            'overall_pass': result['overall_pass']\n",
    "        }\n",
    "        \n",
    "        # Add individual check results\n",
    "        for check_name, check_result in result['evaluation_results'].items():\n",
    "            row[f'{check_name}_passed'] = check_result['passed']\n",
    "            row[f'{check_name}_message'] = check_result['message']\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(rows)\n",
    "    results_df.to_csv(filepath, index=False)\n",
    "    print(f\"Results exported to {filepath}\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# --- Updated naming scheme ---\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "model_str = MODEL.replace(\":\", \"_\").replace(\"/\", \"_\")  # clean up for filenames\n",
    "\n",
    "results_csv_path = RESULTS_DIR / f\"evaluation_results_{model_str}_{date_str}.csv\"\n",
    "results_df = export_results_to_csv(all_results, results_csv_path)\n",
    "\n",
    "print(f\"\\nAnalysis summary saved to {analysis_path}\")\n",
    "print(f\"Results CSV shape: {results_df.shape}\")\n",
    "results_df.head()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
